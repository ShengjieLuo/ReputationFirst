{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "As described in the design part above, the overall idea is to first preprocess raw dataset we get to produce their abstract or simply original text. Then the problem becomes how to use these texts to reflect companies' reputation. One straight forward idea is to use sentiment analysis. However, we met several practical problems.\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "Sentiment analysis here refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n",
    "\n",
    "#### Why sentiment analyis\n",
    "\n",
    "Generally speaking, sentiment analysis aims to determine the attitude of a speaker, writer, or other subject with respect to some topic or the overall contextual polarity or emotional reaction to a document, interaction, or event. The attitude may be a judgment or evaluation (see appraisal theory), affective state (that is to say, the emotional state of the author or speaker), or the intended emotional communication (that is to say, the emotional effect intended by the author or interlocutor).\n",
    "\n",
    "In our application, we use attitude as our main target of sentiment analysis. This is because we want to know attitudes that general trend of popular media holds toward certain company, which helps to reflect the overall reputation.\n",
    "\n",
    "#### Details with attitude analysis\n",
    "There are several different component when analyzing the attitude in each articles. We need to determine: 1) The holder (source) of the attitude. 2) The aspect (target) of the attitude. 3) The detailed type of attitude, including different positive and negtive words and weighting between them. 4) The scope of certain type attitude.\n",
    "\n",
    "#### Bag of words --- Input of Sentiment Analysis\n",
    "We use the simplest model to do sentiment analysis, which is just input the adjecent words with the company name. We have also tried to use EM model to refine this input model. However, one problem is that in each article, the words that appears in the bag is very sparse. In other words, it's hard to use posterior knowledge to refine this model.\n",
    "\n",
    "Here is the full implementation of our Spark program when doing sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pyspark\n",
    "\n",
    "def get_result_list(lines):\n",
    "    import nltk\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    nltk.download('punkt',download_dir='./nltk_data')\n",
    "    nltk.download('vader_lexicon',download_dir='./nltk_data')\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    result_list = []\n",
    "    for line in lines:\n",
    "        json_data = json.loads(line)\n",
    "        \n",
    "        #mode1: major-abstract\n",
    "        if len(json_data[\"abstract\"])>0:\n",
    "            score = sia.polarity_scores(json_data[\"abstract\"])\n",
    "        else:\n",
    "            score = sia.polarity_scores(json_data[\"text\"])\n",
    "        json_data[\"score_abstract\"] = score\n",
    "        \n",
    "        #mode2: text-based\n",
    "        score2 = sia.polarity_scores(json_data[\"text\"])\n",
    "        json_data[\"score_text\"] = score2\n",
    "\n",
    "        #mode3: sentence-based\n",
    "        scores = []\n",
    "        sents  = nltk.sent_tokenize(json_data[\"text\"].lower())\n",
    "        name   = json_data[\"company\"]\n",
    "        for sent in sents:\n",
    "            if name in sent:\n",
    "                scores.append(sia.polarity_scores(sent))\n",
    "        try:\n",
    "            pos_score = 0\n",
    "            neg_score = 0\n",
    "            neu_score = 0\n",
    "            for score in scores:\n",
    "                pos_score += score[\"pos\"]\n",
    "                neg_score += score[\"neg\"]\n",
    "                neu_score += score[\"neu\"]\n",
    "            pos_score /= len(scores)\n",
    "            neg_score /= len(scores)\n",
    "            neu_score /= len(scores)\n",
    "            json_data[\"score_sentence\"] = {\"pos\":pos_score,\"neg\":neg_score,\"neu\":neu_score}\n",
    "        except:\n",
    "            json_data[\"score_sentence\"] = score2\n",
    "\n",
    "        result_list.append(json.dumps(json_data))\n",
    "    return result_list\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    sc = pyspark.SparkContext()\n",
    "    dataRDD = sc.textFile(\"gs://group688/688v3/*\")\n",
    "    dataRDD.mapPartitions(get_result_list).saveAsTextFile(\"gs://group688/688v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreferrence Resolution\n",
    "Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction.\n",
    "\n",
    "#### Why coreferrence resolution\n",
    "When we are exploring news articles we get from various sources, one interesting fact is that most of the time, the company's name may appear only a few times even if it's the main character in the article. And most of other appearance may be 'it', 'the company' and even its CEO or chairman. This lead to the problem that the number of candidates is too small when we use bag of words algorithm to do sentiment analysis of a certain article, if we just use the company name as key word without resolution of these references. \n",
    "\n",
    "#### Usage of coreference resolution\n",
    "Here we uses Stanford Core NLP toolkits to help us pre-process the coreferences. One problem to use the toolkit is that it's written in Java and only has limited support for Python. So we build a local NLP server with following command:\n",
    "\n",
    "```\n",
    "wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip\n",
    "```\n",
    "Then install Python library to access the nlp sever:\n",
    "```\n",
    "pip install stanfordcorenlp\n",
    "```\n",
    "Go to root directory of the downloaded directory, and then run the following command to set up local stanford CoreNLP server (detailed configuration can be found here: https://stanfordnlp.github.io/CoreNLP/history.html):\n",
    "```\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "```\n",
    "Finally, we can access both from web browser through http://localhost:9000 or use the Programming API like following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-137fedddf0ef>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-137fedddf0ef>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print 'Tokenize:', nlp.word_tokenize(sentence)\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP(\"http://localhost\", 9000)\n",
    "\n",
    "sentence = 'Google is a good company'\n",
    "print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "print('Constituency Parsing:', nlp.parse(sentence))\n",
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "\n",
    "nlp.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code above just shows basic operations supported. In order to do coreference resolution, we uses the following pseudo code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP(\"http://localhost\", 9000)\n",
    "f = open(\"dataset\", 'r')\n",
    "# input the whole article and output all possible coreferences \n",
    "result = []\n",
    "while line in f.readlines():\n",
    "    article = json.load(line)\n",
    "    result = nlp.coref(article[\"body\"])\n",
    "# result is a list of list\n",
    "for subject in result:\n",
    "    # If refers to the company name, then put into sentiment analysis\n",
    "    judge_if_company_name()\n",
    "nlp.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "After sentiment analysis, there are 3 scores for each article, which describe the possibility that it's positive, negative or neutral. In order to verify our idea and implementation, we draws several plots for each company to reflect its reputation. \n",
    "\n",
    "![facebook.png](facebook.jpeg)\n",
    "![goole.png](google.jpeg)\n",
    "![amazon.png](amazon.jpeg)\n",
    "\n",
    "Although the overall performance is not very good, \n",
    "![facebook429.png](facebook429.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_fb = company_dict['facebook']\n",
    "print(len(pd_fb))\n",
    "#print(pd_fb.to_string())\n",
    "#print(pd_fb.groupby('date').mean().to_string())\n",
    "pd_tmp = pd_fb.groupby(\"date\").mean().reset_index()\n",
    "#pd_tmp = pd_tmp.assign(std_dev = pd_fb.groupby(\"date\").agg(np.std, ddof = 0).loc[:, \"neg_abstract\"])\n",
    "#pd_fb.groupby(\"date\").agg(np.std, ddof = 0).loc[:, \"neg_abstract\"].to_frame()\n",
    "labels = ['neg_abstract',\\\n",
    "            'pos_abstract',\\\n",
    "            'neg_text',\\\n",
    "            'pos_text',\\\n",
    "            'neg_sentence',\\\n",
    "            'pos_sentence' \\\n",
    "        ]\n",
    "#pd_dev = pd_fb.groupby(\"date\").agg(np.std, ddof = 0).loc[:, \"neg_abstract\"].to_frame().rename(columns={'date': 'date', 'neg_abstract': 'std_dev'})\n",
    "#print(pd_dev.to_string())\n",
    "pd_min = pd_fb.groupby(\"date\").min()\n",
    "pd_max = pd_fb.groupby(\"date\").max()\n",
    "#xmajorLocator = MultipleLocator(10)\n",
    "for label in labels:\n",
    "    if label != \"pos_sentence\":\n",
    "        continue\n",
    "    pd_min_tmp = pd_min.loc[:, label].to_frame().rename(columns={'date': 'date', label: label + \"min\"})\n",
    "    #print(pd_min_tmp.to_string())\n",
    "    pd_tmp = pd_tmp.set_index(\"date\").join(pd_min_tmp).reset_index()\n",
    "    pd_max_tmp = pd_max.loc[:, label].to_frame().rename(columns={'date': 'date', label: label + \"max\"})\n",
    "    #print(pd_max_tmp.to_string())\n",
    "    pd_tmp = pd_tmp.set_index(\"date\").join(pd_max_tmp).reset_index()\n",
    "    pd_tmp = pd_tmp.loc[110:120, :]\n",
    "    dev = [pd_tmp.loc[:, label] - pd_tmp.loc[:, label + \"min\"], pd_tmp.loc[:, label + \"max\"] - pd_tmp.loc[:, label]]\n",
    "    www_plot = plt.subplot(121)\n",
    "    plt.ylim(0, 0.3)\n",
    "    #plt.ylabel(\"A\")\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.errorbar(pd_tmp.date, pd_tmp.loc[:, label], yerr = dev, fmt='k-', ecolor='gray', lw=1)\n",
    "    #www_plot.xaxis.set_major_locator(xmajorLocator)\n",
    "    plt.show()\n",
    "\n",
    "print(pd_tmp.to_string())\n",
    "#print(pd_tmp.to_string())\n",
    "#plt.errorbar(pd_tmp.index, )\n",
    "#plt.errorbar(pd_tmp.index, pd_tmp.loc[:, label], yerr = pd_tmp.loc[:, label + \"min\"])\n",
    "#pd_fb.groupby('date').plot()\n",
    "#pd_fb['date'] = pd.to_datetime(pd_fb['date'])\n",
    "#mask = (pd_fb['date'] > '2018-3-19') & (pd_fb['date'] <= '2018-4-1')\n",
    "#print(pd_fb.loc[mask]['abstract'].to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
