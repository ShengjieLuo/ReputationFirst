{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reputation First\n",
    "## Public Opinion Research based on Media Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "There's many important factors that a technique compnay should care about. Such as human resource, the technique trend, management, operations and so on. As a manager of a company, one should take all those factor into consideration to make money, and these factors should be studied by the manager. Among all these factors, there's one very important one but has rarely been researched, that is the public report and reputation.\n",
    "\n",
    "Why public report can impact a company so much. The reason is easy, as the developement of communication tools and the media techniques. Every one can be exported to a lot of pubilc information from Internet, newspapers and even self-media such as Facebook and Twitter poster, which means once a event has been public or reported, it will spread all over the world in few seconds. As people receives these information, they will read and think about the news and make their own judgement, investment decision, and emotion to these news and the companies involved.\n",
    "\n",
    "The opinions from all readers make a general response to a news, and this response will impact the company, a fomous example recently is that Facebook admitted data leak hits 87 million users, widening privacy scandal. When this event is public, people's trust in Facebook is badly hurt and many people started to delete Facebook application from their cell phone, which further cause the stock price of Facebook decrease a lot. Another positive example is also about Facebook, when Facebook admitted its own fault and announced that it will cooperate with British government to protect users' data from third-party companies, this operation has been reported positively and the readers start to forgive and trust Facebook again, therefore, their stock price start to increase.\n",
    "\n",
    "From these examples we can see that when people do not know that Facebook was leaking their information, their's nothing special happened although actually Facebook was doing bad things. People started to give response when this event has been public and reported by media. We can make this judgement that the reputation of a company is deeply related to the news about this company. If there's always good news from all media about one company, it's impossible that this company has a bad reputation, vice versa.\n",
    "\n",
    "Based on this assumption, we think it will be very interesting to collect and analyse the news of every company in this world, and conclude their reputation from these news. If we can do that automatically, from a manager's aspect, one can just use the news as the input and get their reputation today, and make decisions according to it. From a investor's aspect, one can make investment decision according to this company's reputation by simply use our tool to analyse news about one company. For a normal user, well, one can still know what others and public media thinks about one company. It's a very useful and excited way to build a bridge from public news and the company's reputation, and this idea motivat us to dive deeply into this interesting problem.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we designed, implemented ReputationFirst, a public opinion research system based on public media. Our goal is to build a useful system that can collect, process and analyse public opinions about companies based on public media efficiently, and calculate these companies' reputation based on NLP techniques.\n",
    "\n",
    "In this project, our contributions is first, we collect multiple source of news and extract news about technique companies efficiently, second, we take advantage of Google Cloud Platform and Spark to speed up ETL of the raw data, third, we use TextRank algorithm and NLTK sentiment analysis tools to calculate the reputation score of each technique company.\n",
    "\n",
    "Since there's no ground truth for reputation, we evaluate our system by multiple factors such as significant change of stock price and big event/news happend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Internet Crawler\n",
    "In this project, we use distributed Internet Spider to crawl media articles from different sources. However, it is not easy to crawl the history data from media website directly for following reasons.\n",
    "\n",
    "+ Not only article links are listed in the website, also you could find the advertisement links, recommendation links and so on.\n",
    "+ The website content is customized based on userâ€™s habit and varies from different users in each single view.\n",
    "+ The anti-crawler is setup for these popular websites to avoid an overflow network traffic.\n",
    "\n",
    "Based on the observation above, we have to give up the attempts of fetching contents from the original websites.\n",
    "\n",
    "However, it is important to note that saome popular newspapers and magazines provide the **archive database** for users publicly, which is usually the printable and readable version of articles. Compared with the website content, it provides following features:\n",
    "+ It provides all articles published without customer bias.\n",
    "+ There is no or much less advertisement and other links.\n",
    "+ The content could be fetched from the html webpage directly by beautiful soup\n",
    "+ Usually, the content is marked with clear timestamps.\n",
    "\n",
    "Following are the archives for popular medias, we take the content from May.01 as an example here. Note that usually only the traditional media provides the archive as a summarization of past articles, while internet media or web media does not provide similar service.\n",
    "\n",
    "**TypeA: Newspaper & Maganizes**\n",
    "\n",
    "*Newyork Times*\n",
    "  + http://spiderbites.nytimes.com/2018/articles_2018_05_00000.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "\n",
    "*Wall Street Journals*\n",
    "  + http://www.wsj.com/public/page/archive-2018-5-01.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "\n",
    "*Washington Post*\n",
    "  + http://www.washingtonpost.com/wp-adv/archives/copyright.htm\n",
    "  + The archive is not timestamped, instead key word or topic based.\n",
    "\n",
    "**TypeB: News Agency**\n",
    "\n",
    "*BBC*: \n",
    "  + http://dracos.co.uk/made/bbc-news-archive/2018/05/01/\n",
    "  + Collected by third-party organization with less reliability. Also it does not include all contents of the media.\n",
    "  \n",
    "*Reuters*\n",
    "  + https://uk.reuters.com/resources/archive/uk/20180501.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "  \n",
    "*CNN*\n",
    "  + CNN provides clear archive before 2001 with good format, however, the following materials are really confused without spicification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newspaper crawler is shown as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import threading\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NewsParser: The Newspaper Crawler Class\n",
    "'''\n",
    "class NewsParser(threading.Thread):\n",
    "\n",
    "    def __init__(self,source):\n",
    "        '''\n",
    "        Init Function:\n",
    "        1. Iniralization as a python thread\n",
    "        2. Define the source used in the parser\n",
    "        '''\n",
    "        threading.Thread.__init__(self)\n",
    "        self.source = source\n",
    "\n",
    "    def _fetch_links(self,url):\n",
    "        '''\n",
    "        1. Fetch the url links from archive database\n",
    "        2. Parse the database page to get the article links\n",
    "        '''\n",
    "        links = []\n",
    "        r = requests.get(url)\n",
    "        if r.status_code!=200:\n",
    "            print(url+\":Not Correct Response\")\n",
    "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "        for link in soup.find_all('a'):\n",
    "            rawlink = link.get('href')\n",
    "            if re.match(\"(\\S)+://www.nytimes.com/20(\\d)+/(\\d)+/(\\d)+/(\\S)+\\.html\",rawlink):\n",
    "                links.append(rawlink)\n",
    "        return links\n",
    "\n",
    "    def _fetch_data(self,link):\n",
    "        '''\n",
    "        1. Get the articles and parse the articles by \"newspaper\" lib\n",
    "        2. Add the realted field into the datum struct\n",
    "        '''\n",
    "        datum = {}\n",
    "        article = Article(link)\n",
    "        try:\n",
    "            article.download()\n",
    "        except:\n",
    "            print(link+\": Cannot be download!\")\n",
    "            return datum\n",
    "        try:       \n",
    "            article.parse()\n",
    "        except:\n",
    "            print(link+\": Cannot be parsed!\")\n",
    "            return datum            \n",
    "        datum[\"authors\"] = article.authors\n",
    "        datum[\"date\"]    = str(article.publish_date)\n",
    "        datum[\"text\"]    = str(article.text)\n",
    "        datum[\"title\"]   = str(article.title)\n",
    "        return datum\n",
    "    \n",
    "    def parse(self,source):\n",
    "        '''\n",
    "        Main Function to use the crawling function\n",
    "        '''\n",
    "        f = open(\"data_\"+source.split(\"/\")[-1],\"a\")\n",
    "        links = self._fetch_links(source)\n",
    "        count = 0\n",
    "        for link in links:\n",
    "            datum = self._fetch_data(link)\n",
    "            if len(datum)==0:\n",
    "                continue\n",
    "            f.write(json.dumps(datum)+'\\n')\n",
    "            print(\"Fetch news from \" + source + \" : \"+str(count)+\"/\"+str(len(links)))\n",
    "            count += 1\n",
    "        f.close()\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Provide the interactive interface as a python thread\n",
    "        '''\n",
    "        self.parse(self.source)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program shown above is a single thread internet crawler, however, the single thread crawler is too slower to crawl all newspaper articles. Following codes extend it into a multi-thread crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_targets():\n",
    "    '''\n",
    "    Read the \"sourcelist\" file to get the crawling resources    \n",
    "    '''\n",
    "    f = open(\"sourcelist\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "def real_main():\n",
    "    '''\n",
    "    1. Intializes multi-threads based on sources, each of which uses one thread\n",
    "    2. Coordinate multiple threads and wait for working threads end.\n",
    "    '''\n",
    "    sources = _get_targets()\n",
    "    threads = []\n",
    "    for source in sources:\n",
    "        threads.append(NewsParser(source))\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "real_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning by Spark (ETL)\n",
    "\n",
    "In the next step, we use Spark to clean the raw dataset fetched from the crawler following these steps:\n",
    "\n",
    "*Step1.* Filter out all articles of which **the company name is not included**.  Note that we match the company name by python regular expression within a case-insensitve style.\n",
    "\n",
    "*Step2.* For articles remained, we parse the articles to get the **title**, **summary** and **tag** fields. The method to extract the summary from original text would be described in the next chapter.\n",
    "\n",
    "*Step3.* Filter out articles which the company name is not included in title, summary and tag fields.\n",
    "\n",
    "It is important to note that,\n",
    "+ Sometimes the company name has different meanings, for example, the company name *\"Adobe\"* is used to describe a type of house as well which is used in some articles about earthquakes. Therefore, we use the summary function to identify the relation between articles and company names.\n",
    "+ The summary extraction is a time-consuming task with complex algorithm. Hence, we use step1 as a croase filtering to reduce the overall workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Coarse-Grained Data Filter\n",
    "\n",
    "Coarse Data Filter is used to filter out the articles which is not related to the companies in the company list.First, we use nltk to tokenize text into a list of tokens. And then, we compare the tokens with the company list one by one. Finally, we add the tag name into the result stuct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def _init_line(line):\n",
    "    name = line.lower().split()[0]\n",
    "    return (name,line.lower().split())\n",
    "\n",
    "def _init_list(sc):\n",
    "    results = {}\n",
    "    companyRDD = sc.textFile(\"gs://group688/companylist\")\n",
    "    coms = companyRDD.map(_init_line).collect()\n",
    "    for com in coms:\n",
    "        for name in com[1]:\n",
    "              results[name] = com[0]\n",
    "    return results   \n",
    "\n",
    "def _data_filter(lines,company,source):\n",
    "    import nltk\n",
    "    nltk.download('punkt',download_dir='./nltk_data')\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "    results = []\n",
    "    for datum in lines:\n",
    "        data    = json.loads(datum)\n",
    "        authors = data[\"authors\"]\n",
    "        date    = data[\"date\"]\n",
    "        text    = data[\"text\"]\n",
    "        title   = data[\"title\"]\n",
    "        tokens_text  = word_tokenize(text.lower())\n",
    "        tokens_title = word_tokenize(title.lower())\n",
    "        tags = []\n",
    "        for word in text.lower().split():\n",
    "            if word[0]==\"#\":\n",
    "            tags.append(word.lower())\n",
    "        #Stat is a dictionary, key is the company name, and value is the attribute\n",
    "        #attributes: [in_title,title_count,total_count]\n",
    "        stat  = {}\n",
    "        for token in tokens_title:\n",
    "              if token in company:\n",
    "                if company[token] in stat:\n",
    "                    stat[company[token]][0] = True\n",
    "                    stat[company[token]][1] += 1\n",
    "                else:\n",
    "                    stat[company[token]] = [True,1,0]\n",
    "        for token in tokens_text:\n",
    "              if token in company:\n",
    "                if company[token] in stat:\n",
    "                      stat[company[token]][2] += 1\n",
    "                else:\n",
    "                      stat[company[token]] = [False,0,1]\n",
    "        for name in stat:\n",
    "            result = {}\n",
    "            if (source==\"wsj\"):\n",
    "                result[\"date\"]      = date[:5] + '0' + date[5:9]\n",
    "            else:\n",
    "                result[\"date\"]      = date[:10]\n",
    "        result[\"text\"]        = text\n",
    "        result[\"tokens\"]      = tokens_text\n",
    "        result[\"company\"]     = name\n",
    "        result[\"source\"]      = source\n",
    "        result[\"in_title\"]    = stat[name][0]\n",
    "        result[\"title_count\"] = max(stat[name][1],title.lower().count(name))\n",
    "        result[\"total_count\"] = max(stat[name][2],text.lower().count(name))\n",
    "        result[\"title\"]       = title\n",
    "        result[\"authors\"]     = authors\n",
    "        result[\"tags\"]        = tags\n",
    "        results.append((name,json.dumps(result)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we introduced, even the coarse data filter is really time-consuming. In our experiment, we use a single thread data filter running on 300MB raw dataset with 255k articles, and it takes more than 2 hours to complete. To accelerate the execution time in a GB-level dataset, we use the Spark+Yarn platform to execute the program in parallel on a 5 instance cluster.\n",
    "The distributed cluster configuration is:\n",
    "+ **1 Master Node **\n",
    "  1. 4   CPUs\n",
    "  2. 16  GB memory\n",
    "  3. 100 GB storage\n",
    "  \n",
    "+ **4 Worker Nodes **\n",
    "  1. 2  CPUs\n",
    "  2. 12 GB memory\n",
    "  3. 80 GB storage\n",
    " \n",
    " The spark program is included as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_main():\n",
    "    sc = pyspark.SparkContext()\n",
    "    company = _init_list(sc)\n",
    "    dataRDD1 = sc.textFile(\"gs://group688/nytimes\",5)\n",
    "    dataRDD1 = dataRDD1.mapPartitions(lambda x:_data_filter(x,company,\"nytimes\"))\n",
    "    dataRDD2 = sc.textFile(\"gs://group688/wsj\",10)\n",
    "    dataRDD2 = dataRDD2.mapPartitions(lambda x:_data_filter(x,company,\"wsj\"))\n",
    "    dataRDD3 = sc.textFile(\"gs://group688/reuters.dat\",10)\n",
    "    dataRDD3 = dataRDD3.mapPartitions(lambda x:_data_filter(x,company,\"reuters\"))\n",
    "    dataRDD  = dataRDD3.union(dataRDD2).union(dataRDD1)\n",
    "    dataRDD.sortByKey().map(lambda x:x[1]).saveAsTextFile(\"gs://group688/688v1\")\n",
    "\n",
    "real_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support the Spark execution on cloud platform, we uses the **Google Cloud Dataproc** service to deply the spark cluster efficiently. Following is the scipt to combine the spark program with the google dataproc cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil rm -r gs://group688/688v1\n",
    "gcloud dataproc jobs submit pyspark \\\n",
    "--cluster spark688 \\\n",
    "--region us-east1 \\\n",
    "etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Generate Article Summary\n",
    "\n",
    "#### TextRank\n",
    "\n",
    "TextRank is a very popular and accurate extractive text summarization algorithm. Why we need to get summarization from articles? The reason is that the whole text of the news is usually too big for setiment analysis, therefore we need to compress the size and extract useful text from the article, so we choose to take advantage of this text summarization algorithm.\n",
    "\n",
    "TextRank is similar to PageRank. It considers sentences the equivalent of web pages. The probability of going from sentence A to sentence B is equal to the similarity of the 2 sentences, and then simply apply the PageRank algorithm over this sentence graph. By applying this algorithm, we can decrease the size of text signigicantly.\n",
    "\n",
    "![TextRankModel](image/TextRank.png)\n",
    "\n",
    "The following is the sequencial version of the abstract and keyword extraction agorithm. It take advange of a open-source TextRank implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords\n",
    "\n",
    "\n",
    "def get_abstract_keywords(text):\n",
    "    return summarizer.summarize(text), keywords.keywords(text, split=True)\n",
    "\n",
    "\n",
    "def get_result_list(file, num):\n",
    "    line_count = 0\n",
    "    abandon_count = 0\n",
    "    result_list = []\n",
    "    with open(file, 'r') as raw_data:\n",
    "        while line_count != num:\n",
    "            if line_count % 100 == 0:\n",
    "                print(line_count, datetime.datetime.now())\n",
    "            line_count += 1\n",
    "            json_data = json.loads(raw_data.readline())\n",
    "            abstract, keyword = get_abstract_keywords(json_data['text'])\n",
    "            name = json_data['company']\n",
    "            tags_words = list(map(lambda x: x[1:], json_data['tags']))\n",
    "            abstract_words = list(\n",
    "                map(lambda x: x.lower(),\n",
    "                    nltk.tokenize.word_tokenize(abstract)))\n",
    "            title_words = list(\n",
    "                map(lambda x: x.lower(),\n",
    "                    nltk.tokenize.word_tokenize(json_data['title'])))\n",
    "            if abstract != '' and name not in abstract_words and name not in title_words and name not in tags_words:\n",
    "                abandon_count += 1\n",
    "                print(json_data['title'])\n",
    "                print(abandon_count)\n",
    "                continue\n",
    "            json_data['abstract'] = abstract\n",
    "            json_data['keywords'] = keyword\n",
    "            result_list.append(json.dumps(json_data))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the sequential version of algorithm is too slow to run, especially when there's so many raw text, wo we revise it to the Spark version and speed-up the algorithm by 8 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords\n",
    "def generate_summary(text):\n",
    "    abstract  = summarizer.summarize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Fine-Grained Data Filter\n",
    "\n",
    "The fine-grained data filter is the data filter based on the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pyspark\n",
    "\n",
    "def get_result_list(lines):\n",
    "    from summa import summarizer\n",
    "    from summa import keywords\n",
    "    import nltk\n",
    "    nltk.download('punkt',download_dir='./nltk_data')\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "    result_list = []\n",
    "    for line in lines:\n",
    "        json_data = json.loads(line)\n",
    "        text      = json_data[\"text\"]\n",
    "        abstract  = summarizer.summarize(text)\n",
    "        keyword   = keywords.keywords(text, split=True)\n",
    "        name = json_data['company']\n",
    "        tags_words = list(map(lambda x: x[1:], json_data['tags']))\n",
    "        abstract_words = list(map(lambda x: x.lower(), nltk.tokenize.word_tokenize(abstract)))\n",
    "        title_words = list(map(lambda x: x.lower(), nltk.tokenize.word_tokenize(json_data['title'])))\n",
    "        if abstract != '' and name not in abstract_words and name not in title_words and name not in tags_words:\n",
    "            continue\n",
    "        json_data['abstract'] = abstract\n",
    "        json_data['keywords'] = keyword\n",
    "        result_list.append(json.dumps(json_data))\n",
    "    return result_list\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    sc = pyspark.SparkContext()\n",
    "    dataRDD = sc.textFile(\"gs://group688/688v2.dat\",20)\n",
    "    dataRDD.mapPartitions(get_result_list).saveAsTextFile(\"gs://group688/688v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Get Stock Price\n",
    "\n",
    "The last step is to use pandas data interface to get and store the stock price for a list of companies in a specific time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data, wb\n",
    "\n",
    "\n",
    "def get_stock_pdf(ticker):\n",
    "    start = datetime.datetime(2018, 1, 1)\n",
    "    end = datetime.date.today()\n",
    "    print(ticker)\n",
    "    time.sleep(1)\n",
    "    return ticker, data.DataReader(ticker, \"iex\", start, end)\n",
    "\n",
    "\n",
    "def read_stock_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        fl = f.readlines()\n",
    "        tickers = list(filter(lambda x: x, map(lambda x: x.split(',')[1].strip(), fl)))\n",
    "        tickers_map = {i.split(',')[1].strip(): i.split(',')[0].split(' ') for i in fl}\n",
    "        return tickers, tickers_map\n",
    "\n",
    "\n",
    "def output_csv(pdfs):\n",
    "    for pdf in pdfs:\n",
    "        pdf[1].to_csv('data/'+pdf[0]+'.csv', sep=',', encoding='utf-8')\n",
    "        \n",
    "\n",
    "tickers, tickers_map = read_stock_data('ticker_list')\n",
    "print(tickers)\n",
    "pdfs = [get_stock_pdf(t) for t in tickers]\n",
    "output_csv(pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
