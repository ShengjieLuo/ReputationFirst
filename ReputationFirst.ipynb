{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reputation First\n",
    "## -- A Public Opinion Research based on Media Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we designed, implemented ReputationFirst, a public opinion research system based on public media. Our goal is to build a useful system that can collect, process and analyse public opinions about companies based on public media efficiently, and calculate these companies' reputation based on NLP techniques.\n",
    "<img src=\"https://raw.githubusercontent.com/ShengjieLuo/ReputationFirst/master/image/intro.jpg\" width=\"500\">\n",
    "\n",
    "We will cover the following topics in this report:\n",
    "- [1. Motivation](#1.-Motivation)\n",
    "- [2. Contribution](#2.-Contribution)\n",
    "- [3. Architecture](#3.-Architecture)\n",
    "- [4. Implementation](#4.-Implementation)\n",
    "- [5. Evaluation](#5.-Evaluation)\n",
    "- [6. Conclustion](#6.-Conclusion)\n",
    "- [Reference](#Reference)\n",
    "\n",
    "\n",
    "# 1. Motivation\n",
    "\n",
    "There's many important factors that a technique compnay should care about. Such as human resource, the technique trend, management, operations and so on. As a manager of a company, one should take all those factor into consideration to make money, and these factors should be studied by the manager. Among all these factors, there's one very important one but has rarely been researched, that is the **public report and reputation**.\n",
    "\n",
    "Why public report can impact a company so much. The reason is easy, as the developement of communication tools and the media techniques. Every one can be exported to a lot of pubilc information from Internet, newspapers and even self-media such as Facebook and Twitter poster, which means once a event has been public or reported, it will spread all over the world in few seconds. As people receives these information, they will read and think about the news and make their own judgement, investment decision, and emotion to these news and the companies involved.\n",
    "\n",
    "The opinions from all readers make a general response to a news, and this response will impact the company, a famous example is that Facebook admitted data leak hits 87 million users, widening privacy scandal. When this event is public, people's trust in Facebook is badly hurt and many people started to delete Facebook application from their cell phone, which further cause the stock price of Facebook decrease a lot. Another positive example is also about Facebook, when Facebook admitted its own fault and announced that it will cooperate with British government to protect users' data from third-party companies, this operation has been reported positively and the readers start to forgive and trust Facebook again, therefore, their stock price start to increase.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ShengjieLuo/ReputationFirst/master/image/motivation.jpg\" width=\"500\">\n",
    "\n",
    "From these examples we can see that when people do not know that Facebook was leaking their information, their's nothing special happened although actually Facebook was doing bad things. People started to give response when this event has been public and reported by media. We can make this judgement that **the reputation of a company is deeply related to the news about this company**. If there's always good news from all media about one company, it's impossible that this company has a bad reputation, vice versa.\n",
    "\n",
    "Based on this assumption, we think it would be quite interesting to collect and analyze the news of every company in this world, and conclude their reputation from these news. If we can do that automatically, from a manager's aspect, one can just **use the news as the input, get their reputation today, and make decisions according to it**. From a investor's aspect, one can make investment decision according to this company's reputation by simply use our tool to analyse news about one company. For a normal user, well, one can still know what others and public media thinks about one company. It's a very useful and excited way to build a bridge from public news and the company's reputation, and this idea motivates us to dive deeply into this interesting problem.\n",
    "\n",
    "# 2. Contribution\n",
    "\n",
    "In this project, our contributions is \n",
    "1. We collect **multiple sources** of news and extract news about technique companies efficiently by **distributed internet crawler**.\n",
    "2. We take advantage of Google Cloud Platform and **Spark** to speed up ETL and NLP data anlysis.\n",
    "2. We use **TextRank** algorithm and NLTK **sentiment analysis** tools to calculate the reputation score of each technique company.\n",
    "\n",
    "Since there's no ground truth for reputation, we evaluate our system by multiple factors such as significant change of stock price and big event/news happend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Architecture\n",
    "<img src=https://raw.githubusercontent.com/ShengjieLuo/ReputationFirst/master/image/arch.jpg width=\"500\">\n",
    "\n",
    "We use distributed internet spider to crawl articles from Reuters, NewYork Times and Wall Street Journals, after which we use Spark to clean data and apply textrank algorithm to extract the summary from original text. In the next step, python NLTK is a good solution to handle sentiment analysis and extract media attitude. Finally, these messages are combined to predict the reputation score.\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/ShengjieLuo/ReputationFirst/master/image/analysis.jpg width=\"500\">\n",
    "\n",
    "One of the largest challenges is how to process 2GB data and 600 Million tokens efficiently in 3 different tasks, including *ETL* task, *textrank* task and *sentiment analysis* task. \n",
    "\n",
    "It is a typical **Iterative Machine Learning** workload, in which the intermediate result of previous stages would then be used in the next stage. And one of the best method to accelerate the Iterative Machine Learning is to use the **Spark+Yarn+CloudCluster** framework.\n",
    "\n",
    "In our implementation, we use a spark cluster with 8 cores to accelerate the workload from 73.9 hours to 9.8 hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementation\n",
    "\n",
    "In this chapter, we would introduce the implementation of this project.\n",
    "## 4.1 Distributed Internet Crawler\n",
    "In this project, we use distributed Internet Spider to crawl media articles from different sources. However, it is not easy to crawl the history data from media website directly for following reasons.\n",
    "\n",
    "+ Not only article links are listed in the website, also you could find the advertisement links, recommendation links and so on.\n",
    "+ The website content is customized based on userâ€™s habit and varies from different users in each single view.\n",
    "+ The anti-crawler is setup for these popular websites to avoid an overflow network traffic.\n",
    "\n",
    "Based on the observation above, we have to give up the attempts of fetching contents from the original websites.\n",
    "\n",
    "However, it is important to note that saome popular newspapers and magazines provide the **archive database** for users publicly, which is usually the printable and readable version of articles. Compared with the website content, it provides following features:\n",
    "+ It provides all articles published without customer bias.\n",
    "+ There is no or much less advertisement and other links.\n",
    "+ The content could be fetched from the html webpage directly by beautiful soup\n",
    "+ Usually, the content is marked with clear timestamps.\n",
    "\n",
    "Following are the archives for popular medias, we take the content from May.01 as an example here. Note that usually only the traditional media provides the archive as a summarization of past articles, while internet media or web media does not provide similar service.\n",
    "\n",
    "**TypeA: Newspaper & Maganizes**\n",
    "\n",
    "*Newyork Times*\n",
    "  + http://spiderbites.nytimes.com/2018/articles_2018_05_00000.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "\n",
    "*Wall Street Journals*\n",
    "  + http://www.wsj.com/public/page/archive-2018-5-01.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "\n",
    "*Washington Post*\n",
    "  + http://www.washingtonpost.com/wp-adv/archives/copyright.htm\n",
    "  + The archive is not timestamped, instead key word or topic based.\n",
    "\n",
    "**TypeB: News Agency**\n",
    "\n",
    "*BBC*: \n",
    "  + http://dracos.co.uk/made/bbc-news-archive/2018/05/01/\n",
    "  + Collected by third-party organization with less reliability. Also it does not include all contents of the media.\n",
    "  \n",
    "*Reuters*\n",
    "  + https://uk.reuters.com/resources/archive/uk/20180501.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "  \n",
    "*CNN*\n",
    "  + CNN provides clear archive before 2001 with good format, however, the following materials are really confused without spicification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newspaper crawler is shown as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import threading\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NewsParser: The Newspaper Crawler Class\n",
    "'''\n",
    "class NewsParser(threading.Thread):\n",
    "\n",
    "    def __init__(self,source):\n",
    "        '''\n",
    "        Init Function:\n",
    "        1. Iniralization as a python thread\n",
    "        2. Define the source used in the parser\n",
    "        '''\n",
    "        threading.Thread.__init__(self)\n",
    "        self.source = source\n",
    "\n",
    "    def _fetch_links(self,url):\n",
    "        '''\n",
    "        1. Fetch the url links from archive database\n",
    "        2. Parse the database page to get the article links\n",
    "        '''\n",
    "        links = []\n",
    "        r = requests.get(url)\n",
    "        if r.status_code!=200:\n",
    "            print(url+\":Not Correct Response\")\n",
    "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "        for link in soup.find_all('a'):\n",
    "            rawlink = link.get('href')\n",
    "            if re.match(\"(\\S)+://www.nytimes.com/20(\\d)+/(\\d)+/(\\d)+/(\\S)+\\.html\",rawlink):\n",
    "                links.append(rawlink)\n",
    "        return links\n",
    "\n",
    "    def _fetch_data(self,link):\n",
    "        '''\n",
    "        1. Get the articles and parse the articles by \"newspaper\" lib\n",
    "        2. Add the realted field into the datum struct\n",
    "        '''\n",
    "        datum = {}\n",
    "        article = Article(link)\n",
    "        try:\n",
    "            article.download()\n",
    "        except:\n",
    "            print(link+\": Cannot be download!\")\n",
    "            return datum\n",
    "        try:       \n",
    "            article.parse()\n",
    "        except:\n",
    "            print(link+\": Cannot be parsed!\")\n",
    "            return datum            \n",
    "        datum[\"authors\"] = article.authors\n",
    "        datum[\"date\"]    = str(article.publish_date)\n",
    "        datum[\"text\"]    = str(article.text)\n",
    "        datum[\"title\"]   = str(article.title)\n",
    "        return datum\n",
    "    \n",
    "    def parse(self,source):\n",
    "        '''\n",
    "        Main Function to use the crawling function\n",
    "        '''\n",
    "        f = open(\"data_\"+source.split(\"/\")[-1],\"a\")\n",
    "        links = self._fetch_links(source)\n",
    "        count = 0\n",
    "        for link in links:\n",
    "            datum = self._fetch_data(link)\n",
    "            if len(datum)==0:\n",
    "                continue\n",
    "            f.write(json.dumps(datum)+'\\n')\n",
    "            print(\"Fetch news from \" + source + \" : \"+str(count)+\"/\"+str(len(links)))\n",
    "            count += 1\n",
    "        f.close()\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Provide the interactive interface as a python thread\n",
    "        '''\n",
    "        self.parse(self.source)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program shown above is a single thread internet crawler, however, the single thread crawler is too slower to crawl all newspaper articles. Following codes extend it into a multi-thread crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_targets():\n",
    "    '''\n",
    "    Read the \"sourcelist\" file to get the crawling resources    \n",
    "    '''\n",
    "    f = open(\"sourcelist\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "def real_main():\n",
    "    '''\n",
    "    1. Intializes multi-threads based on sources, each of which uses one thread\n",
    "    2. Coordinate multiple threads and wait for working threads end.\n",
    "    '''\n",
    "    sources = _get_targets()\n",
    "    threads = []\n",
    "    for source in sources:\n",
    "        threads.append(NewsParser(source))\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "real_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data Extract&Transformation&Load (ETL)\n",
    "\n",
    "In the next step, we use Spark to clean the raw dataset fetched from the crawler following these steps:\n",
    "\n",
    "*Step1.* Filter out all articles of which **the company name is not included**.  Note that we match the company name by python regular expression within a case-insensitve style.\n",
    "\n",
    "*Step2.* For articles remained, we parse the articles to get the **title**, **summary** and **tag** fields. The method to extract the summary from original text would be described in the next chapter.\n",
    "\n",
    "*Step3.* Filter out articles which the company name is not included in title, summary and tag fields.\n",
    "\n",
    "It is important to note that,\n",
    "+ Sometimes the company name has different meanings, for example, the company name *\"Adobe\"* is used to describe a type of house as well which is used in some articles about earthquakes. Therefore, we use the summary function to identify the relation between articles and company names.\n",
    "+ The summary extraction is a time-consuming task with complex algorithm. Hence, we use step1 as a croase filtering to reduce the overall workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Coarse-Grained Data Filter\n",
    "\n",
    "Coarse Data Filter is used to filter out the articles which is not related to the companies in the company list.First, we use nltk to tokenize text into a list of tokens. And then, we compare the tokens with the company list one by one. Finally, we add the tag name into the result stuct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def _init_line(line):\n",
    "    name = line.lower().split()[0]\n",
    "    return (name,line.lower().split())\n",
    "\n",
    "def _init_list(sc):\n",
    "    results = {}\n",
    "    companyRDD = sc.textFile(\"gs://group688/companylist\")\n",
    "    coms = companyRDD.map(_init_line).collect()\n",
    "    for com in coms:\n",
    "        for name in com[1]:\n",
    "              results[name] = com[0]\n",
    "    return results   \n",
    "\n",
    "def _data_filter(lines,company,source):\n",
    "    import nltk\n",
    "    nltk.download('punkt',download_dir='./nltk_data')\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "    results = []\n",
    "    for datum in lines:\n",
    "        data    = json.loads(datum)\n",
    "        authors = data[\"authors\"]\n",
    "        date    = data[\"date\"]\n",
    "        text    = data[\"text\"]\n",
    "        title   = data[\"title\"]\n",
    "        tokens_text  = word_tokenize(text.lower())\n",
    "        tokens_title = word_tokenize(title.lower())\n",
    "        tags = []\n",
    "        for word in text.lower().split():\n",
    "            if word[0]==\"#\":\n",
    "            tags.append(word.lower())\n",
    "        #Stat is a dictionary, key is the company name, and value is the attribute\n",
    "        #attributes: [in_title,title_count,total_count]\n",
    "        stat  = {}\n",
    "        for token in tokens_title:\n",
    "              if token in company:\n",
    "                if company[token] in stat:\n",
    "                    stat[company[token]][0] = True\n",
    "                    stat[company[token]][1] += 1\n",
    "                else:\n",
    "                    stat[company[token]] = [True,1,0]\n",
    "        for token in tokens_text:\n",
    "              if token in company:\n",
    "                if company[token] in stat:\n",
    "                      stat[company[token]][2] += 1\n",
    "                else:\n",
    "                      stat[company[token]] = [False,0,1]\n",
    "        for name in stat:\n",
    "            result = {}\n",
    "            if (source==\"wsj\"):\n",
    "                result[\"date\"]      = date[:5] + '0' + date[5:9]\n",
    "            else:\n",
    "                result[\"date\"]      = date[:10]\n",
    "        result[\"text\"]        = text\n",
    "        result[\"tokens\"]      = tokens_text\n",
    "        result[\"company\"]     = name\n",
    "        result[\"source\"]      = source\n",
    "        result[\"in_title\"]    = stat[name][0]\n",
    "        result[\"title_count\"] = max(stat[name][1],title.lower().count(name))\n",
    "        result[\"total_count\"] = max(stat[name][2],text.lower().count(name))\n",
    "        result[\"title\"]       = title\n",
    "        result[\"authors\"]     = authors\n",
    "        result[\"tags\"]        = tags\n",
    "        results.append((name,json.dumps(result)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we introduced, even the coarse data filter is really time-consuming. In our experiment, we use a single thread data filter running on 300MB raw dataset with 255k articles, and it takes more than 2 hours to complete. To accelerate the execution time in a GB-level dataset, we use the Spark+Yarn platform to execute the program in parallel on a 5 instance cluster.\n",
    "The distributed cluster configuration is:\n",
    "+ **1 Master Node **\n",
    "  1. 4   CPUs\n",
    "  2. 16  GB memory\n",
    "  3. 100 GB storage\n",
    "  \n",
    "+ **4 Worker Nodes **\n",
    "  1. 2  CPUs\n",
    "  2. 12 GB memory\n",
    "  3. 80 GB storage\n",
    " \n",
    " The spark program is included as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_main():\n",
    "    sc = pyspark.SparkContext()\n",
    "    company = _init_list(sc)\n",
    "    dataRDD1 = sc.textFile(\"gs://group688/nytimes\",5)\n",
    "    dataRDD1 = dataRDD1.mapPartitions(lambda x:_data_filter(x,company,\"nytimes\"))\n",
    "    dataRDD2 = sc.textFile(\"gs://group688/wsj\",10)\n",
    "    dataRDD2 = dataRDD2.mapPartitions(lambda x:_data_filter(x,company,\"wsj\"))\n",
    "    dataRDD3 = sc.textFile(\"gs://group688/reuters.dat\",10)\n",
    "    dataRDD3 = dataRDD3.mapPartitions(lambda x:_data_filter(x,company,\"reuters\"))\n",
    "    dataRDD  = dataRDD3.union(dataRDD2).union(dataRDD1)\n",
    "    dataRDD.sortByKey().map(lambda x:x[1]).saveAsTextFile(\"gs://group688/688v1\")\n",
    "\n",
    "real_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support the Spark execution on cloud platform, we uses the **Google Cloud Dataproc** service to deply the spark cluster efficiently. Following is the scipt to combine the spark program with the google dataproc cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil rm -r gs://group688/688v1\n",
    "gcloud dataproc jobs submit pyspark \\\n",
    "--cluster spark688 \\\n",
    "--region us-east1 \\\n",
    "etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Generate Article Summary\n",
    "\n",
    "#### TextRank\n",
    "\n",
    "TextRank is a very popular and accurate extractive text summarization algorithm. Why we need to get summarization from articles? The reason is that the whole text of the news is usually too big for setiment analysis, therefore we need to compress the size and extract useful text from the article, so we choose to take advantage of this text summarization algorithm.\n",
    "\n",
    "TextRank is similar to PageRank. It considers sentences the equivalent of web pages. The probability of going from sentence A to sentence B is equal to the similarity of the 2 sentences, and then simply apply the PageRank algorithm over this sentence graph. By applying this algorithm, we can decrease the size of text signigicantly.\n",
    "\n",
    "![TextRankModel](image/TextRank.png)\n",
    "\n",
    "The following is the sequencial version of the abstract and keyword extraction agorithm. It take advange of a open-source TextRank implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords\n",
    "\n",
    "\n",
    "def get_abstract_keywords(text):\n",
    "    return summarizer.summarize(text), keywords.keywords(text, split=True)\n",
    "\n",
    "\n",
    "def get_result_list(file, num):\n",
    "    line_count = 0\n",
    "    abandon_count = 0\n",
    "    result_list = []\n",
    "    with open(file, 'r') as raw_data:\n",
    "        while line_count != num:\n",
    "            if line_count % 100 == 0:\n",
    "                print(line_count, datetime.datetime.now())\n",
    "            line_count += 1\n",
    "            json_data = json.loads(raw_data.readline())\n",
    "            abstract, keyword = get_abstract_keywords(json_data['text'])\n",
    "            name = json_data['company']\n",
    "            tags_words = list(map(lambda x: x[1:], json_data['tags']))\n",
    "            abstract_words = list(\n",
    "                map(lambda x: x.lower(),\n",
    "                    nltk.tokenize.word_tokenize(abstract)))\n",
    "            title_words = list(\n",
    "                map(lambda x: x.lower(),\n",
    "                    nltk.tokenize.word_tokenize(json_data['title'])))\n",
    "            if abstract != '' and name not in abstract_words and name not in title_words and name not in tags_words:\n",
    "                abandon_count += 1\n",
    "                print(json_data['title'])\n",
    "                print(abandon_count)\n",
    "                continue\n",
    "            json_data['abstract'] = abstract\n",
    "            json_data['keywords'] = keyword\n",
    "            result_list.append(json.dumps(json_data))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the sequential version of algorithm is too slow to run, especially when there's so many raw text, wo we revise it to the Spark version and speed-up the algorithm by 8 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords\n",
    "def generate_summary(text):\n",
    "    abstract  = summarizer.summarize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Fine-Grained Data Filter\n",
    "\n",
    "The fine-grained data filter is the data filter based on the generated summary. The reason why we introduce the fine-grained data filter is that some articles only cover the company name in few details, while the major content has no relation to this company.Considering this user case, the company name would no longer be included in the summary generation, therefore, we could filter the unrelated articles in a more fine-grained level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pyspark\n",
    "\n",
    "def get_result_list(lines):\n",
    "    from summa import summarizer\n",
    "    from summa import keywords\n",
    "    import nltk\n",
    "    nltk.download('punkt',download_dir='./nltk_data')\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "    result_list = []\n",
    "    for line in lines:\n",
    "        json_data = json.loads(line)\n",
    "        text      = json_data[\"text\"]\n",
    "        abstract  = summarizer.summarize(text)\n",
    "        keyword   = keywords.keywords(text, split=True)\n",
    "        name = json_data['company']\n",
    "        tags_words = list(map(lambda x: x[1:], json_data['tags']))\n",
    "        abstract_words = list(map(lambda x: x.lower(), nltk.tokenize.word_tokenize(abstract)))\n",
    "        title_words = list(map(lambda x: x.lower(), nltk.tokenize.word_tokenize(json_data['title'])))\n",
    "        if abstract != '' and name not in abstract_words and name not in title_words and name not in tags_words:\n",
    "            continue\n",
    "        json_data['abstract'] = abstract\n",
    "        json_data['keywords'] = keyword\n",
    "        result_list.append(json.dumps(json_data))\n",
    "    return result_list\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    sc = pyspark.SparkContext()\n",
    "    dataRDD = sc.textFile(\"gs://group688/688v2.dat\",20)\n",
    "    dataRDD.mapPartitions(get_result_list).saveAsTextFile(\"gs://group688/688v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the step4.2 ETL, we get a series of json data structs, each of which presents a \"company-article\" relation. The json struct is defined as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = {\n",
    "  \"company\":\"google\",\n",
    "  \"is_title\":True,\n",
    "  \"title_count\":1,\n",
    "  \"text_count\":8,\n",
    "  \"title\": \"is google a better engine than baidu\",\n",
    "  \"text\": \"qwertyuiopsdfghjklertyuiopdfghjkl;ertyuiolp;dfghjklfvgbhnjmk\",\n",
    "  \"tokens\": [\"is\",\"google\",\"better\", ...],\n",
    "  \"date\": \"2018-05-01\",\n",
    "  \"source\": \"nytimes\",\n",
    "  \"tags\":[\"#nytoday\",\"#deletefacebook\"],\n",
    "  \"authors\":[\"John Williams\",\"name2\",\"name3\"],\n",
    "  \"abstract\":\"google is better than baidu in most respects.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Get Stock Price\n",
    "\n",
    "The last step is to use pandas data interface to get and store the stock price for a list of companies in a specific time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data, wb\n",
    "\n",
    "\n",
    "def get_stock_pdf(ticker):\n",
    "    start = datetime.datetime(2018, 1, 1)\n",
    "    end = datetime.date.today()\n",
    "    print(ticker)\n",
    "    time.sleep(1)\n",
    "    return ticker, data.DataReader(ticker, \"iex\", start, end)\n",
    "\n",
    "\n",
    "def read_stock_data(file):\n",
    "    with open(file, 'r') as f:\n",
    "        fl = f.readlines()\n",
    "        tickers = list(filter(lambda x: x, map(lambda x: x.split(',')[1].strip(), fl)))\n",
    "        tickers_map = {i.split(',')[1].strip(): i.split(',')[0].split(' ') for i in fl}\n",
    "        return tickers, tickers_map\n",
    "\n",
    "\n",
    "def output_csv(pdfs):\n",
    "    for pdf in pdfs:\n",
    "        pdf[1].to_csv('data/'+pdf[0]+'.csv', sep=',', encoding='utf-8')\n",
    "        \n",
    "\n",
    "tickers, tickers_map = read_stock_data('ticker_list')\n",
    "print(tickers)\n",
    "pdfs = [get_stock_pdf(t) for t in tickers]\n",
    "output_csv(pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4  Sentiment Analysis\n",
    "\n",
    "As described in the design part above, the overall idea is to first preprocess raw dataset we get to produce their abstract or simply original text. Then the problem becomes how to use these texts to reflect companies' reputation. One straight forward idea is to use sentiment analysis. However, we met several practical problems.\n",
    "\n",
    "Sentiment analysis here refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n",
    "\n",
    "### Why sentiment analyis\n",
    "\n",
    "Generally speaking, sentiment analysis aims to determine the attitude of a speaker, writer, or other subject with respect to some topic or the overall contextual polarity or emotional reaction to a document, interaction, or event. The attitude may be a judgment or evaluation (see appraisal theory), affective state (that is to say, the emotional state of the author or speaker), or the intended emotional communication (that is to say, the emotional effect intended by the author or interlocutor).\n",
    "\n",
    "In our application, we use attitude as our main target of sentiment analysis. This is because we want to know attitudes that general trend of popular media holds toward certain company, which helps to reflect the overall reputation.\n",
    "\n",
    "### Details with attitude analysis\n",
    "There are several different component when analyzing the attitude in each articles. We need to determine: 1) The holder (source) of the attitude. 2) The aspect (target) of the attitude. 3) The detailed type of attitude, including different positive and negtive words and weighting between them. 4) The scope of certain type attitude.\n",
    "\n",
    "### Bag of words --- Input of Sentiment Analysis\n",
    "We use the simplest model to do sentiment analysis, which is just input the adjecent words with the company name. We have also tried to use EM model to refine this input model. However, one problem is that in each article, the words that appears in the bag is very sparse. In other words, it's hard to use posterior knowledge to refine this model.\n",
    "\n",
    "Here is the full implementation of our Spark program when doing sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pyspark\n",
    "\n",
    "def get_result_list(lines):\n",
    "    import nltk\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    nltk.download('punkt',download_dir='./nltk_data')\n",
    "    nltk.download('vader_lexicon',download_dir='./nltk_data')\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    result_list = []\n",
    "    for line in lines:\n",
    "        json_data = json.loads(line)\n",
    "        \n",
    "        #mode1: major-abstract\n",
    "        if len(json_data[\"abstract\"])>0:\n",
    "            score = sia.polarity_scores(json_data[\"abstract\"])\n",
    "        else:\n",
    "            score = sia.polarity_scores(json_data[\"text\"])\n",
    "        json_data[\"score_abstract\"] = score\n",
    "        \n",
    "        #mode2: text-based\n",
    "        score2 = sia.polarity_scores(json_data[\"text\"])\n",
    "        json_data[\"score_text\"] = score2\n",
    "\n",
    "        #mode3: sentence-based\n",
    "        scores = []\n",
    "        sents  = nltk.sent_tokenize(json_data[\"text\"].lower())\n",
    "        name   = json_data[\"company\"]\n",
    "        for sent in sents:\n",
    "            if name in sent:\n",
    "                scores.append(sia.polarity_scores(sent))\n",
    "        try:\n",
    "            pos_score = 0\n",
    "            neg_score = 0\n",
    "            neu_score = 0\n",
    "            for score in scores:\n",
    "                pos_score += score[\"pos\"]\n",
    "                neg_score += score[\"neg\"]\n",
    "                neu_score += score[\"neu\"]\n",
    "            pos_score /= len(scores)\n",
    "            neg_score /= len(scores)\n",
    "            neu_score /= len(scores)\n",
    "            json_data[\"score_sentence\"] = {\"pos\":pos_score,\"neg\":neg_score,\"neu\":neu_score}\n",
    "        except:\n",
    "            json_data[\"score_sentence\"] = score2\n",
    "\n",
    "        result_list.append(json.dumps(json_data))\n",
    "    return result_list\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    sc = pyspark.SparkContext()\n",
    "    dataRDD = sc.textFile(\"gs://group688/688v3/*\")\n",
    "    dataRDD.mapPartitions(get_result_list).saveAsTextFile(\"gs://group688/688v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Coreferrence Resolution\n",
    "Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction.\n",
    "\n",
    "#### Why coreferrence resolution\n",
    "When we are exploring news articles we get from various sources, one interesting fact is that most of the time, the company's name may appear only a few times even if it's the main character in the article. And most of other appearance may be 'it', 'the company' and even its CEO or chairman. This lead to the problem that the number of candidates is too small when we use bag of words algorithm to do sentiment analysis of a certain article, if we just use the company name as key word without resolution of these references. \n",
    "\n",
    "#### Usage of coreference resolution\n",
    "Here we uses Stanford Core NLP toolkits to help us pre-process the coreferences. One problem to use the toolkit is that it's written in Java and only has limited support for Python. So we build a local NLP server with following command:\n",
    "\n",
    "```\n",
    "wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-02-27.zip\n",
    "```\n",
    "Then install Python library to access the nlp sever:\n",
    "```\n",
    "pip install stanfordcorenlp\n",
    "```\n",
    "Go to root directory of the downloaded directory, and then run the following command to set up local stanford CoreNLP server (detailed configuration can be found here: https://stanfordnlp.github.io/CoreNLP/history.html):\n",
    "```\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000\n",
    "```\n",
    "Finally, we can access both from web browser through http://localhost:9000 or use the Programming API like following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP(\"http://localhost\", 9000)\n",
    "\n",
    "sentence = 'Google is a good company'\n",
    "print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "print('Constituency Parsing:', nlp.parse(sentence))\n",
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "\n",
    "nlp.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code above just shows basic operations supported. In order to do coreference resolution, we uses the following pseudo code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP(\"http://localhost\", 9000)\n",
    "f = open(\"dataset\", 'r')\n",
    "# input the whole article and output all possible coreferences \n",
    "result = []\n",
    "while line in f.readlines():\n",
    "    article = json.load(line)\n",
    "    result = nlp.coref(article[\"body\"])\n",
    "# result is a list of list\n",
    "for subject in result:\n",
    "    # If refers to the company name, then put into sentiment analysis\n",
    "    judge_if_company_name()\n",
    "nlp.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Combined Prediction\n",
    "\n",
    "After the 5 steps above, we have calculated the reputation score for each \"company-article\" reflection, the final step is to combine the prediction together and build the dataframe for prediction result. Following codes indicate the procedure for final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_dict = {}\n",
    "def get_company_dataframes(file):\n",
    "    line_count = 0\n",
    "    abandon_count = 0\n",
    "    result_list = []\n",
    "    with open(file, 'r') as raw_data:\n",
    "        while True:\n",
    "            line = raw_data.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line_count % 1000 == 0:\n",
    "                print(line_count, datetime.datetime.now())\n",
    "            line_count += 1\n",
    "            json_data = json.loads(line)\n",
    "            name = json_data['company']\n",
    "            d = {'neg_abstract': json_data['score_abstract']['neg'],\\\n",
    "                 'pos_abstract': json_data['score_abstract']['pos'],\\\n",
    "                 'neu_abstract': json_data['score_abstract']['neu'],\\\n",
    "                 'neg_text': json_data['score_text']['neg'],\\\n",
    "                 'pos_text': json_data['score_text']['pos'],\\\n",
    "                 'neu_text': json_data['score_text']['neu'],\\\n",
    "                 'neg_sentence': json_data['score_sentence']['neg'],\\\n",
    "                 'pos_sentence': json_data['score_sentence']['pos'],\\\n",
    "                 'neu_sentence': json_data['score_sentence']['neu'],\\\n",
    "                 'date': json_data['date'],\\\n",
    "                 'week': json_data['week']}\n",
    "            if name not in company_dict:\n",
    "                company_dict[name] = pd.DataFrame([d])\n",
    "            else:\n",
    "                df = pd.DataFrame([d])\n",
    "                company_dict[name] = company_dict[name].append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is presenteded as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_company_dataframes('688v43.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0 2018-05-08 23:11:38.375655\n",
    "1000 2018-05-08 23:11:41.527542\n",
    "2000 2018-05-08 23:11:44.186069\n",
    "3000 2018-05-08 23:11:46.843435\n",
    "4000 2018-05-08 23:11:49.595050\n",
    "5000 2018-05-08 23:11:52.346185\n",
    "6000 2018-05-08 23:11:54.849394\n",
    "7000 2018-05-08 23:11:57.364961\n",
    "8000 2018-05-08 23:11:59.890049\n",
    "9000 2018-05-08 23:12:02.564109\n",
    "10000 2018-05-08 23:12:05.154044\n",
    "```\n",
    "Let's take the company **facebook** as an example here. In following script, we get all articles related to facebook from the dataframe, and then count the number of these articles. Totally, we get 2080 articles related to facebook, we would use this result in the next step to present the effect of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_fbpd_fb  ==  company_dictcompany_ ['facebook']\n",
    "print(len(pd_fb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n",
    "After sentiment analysis, there are 3 scores for each article, which describe the possibility that it's positive, negative or neutral. In order to verify our idea and implementation, we draws several plots for each company to reflect its reputation. \n",
    "\n",
    "The visualization program is indicated as following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_fb = company_dict['facebook']\n",
    "print(len(pd_fb))\n",
    "pd_tmp = pd_fb.groupby(\"date\").mean().reset_index()\n",
    "labels = ['neg_abstract',\\\n",
    "            'pos_abstract',\\\n",
    "            'neg_text',\\\n",
    "            'pos_text',\\\n",
    "            'neg_sentence',\\\n",
    "            'pos_sentence' \\\n",
    "        ]\n",
    "\n",
    "pd_min = pd_fb.groupby(\"date\").min()\n",
    "pd_max = pd_fb.groupby(\"date\").max()\n",
    "#xmajorLocator = MultipleLocator(10)\n",
    "for label in labels:\n",
    "    if label != \"pos_sentence\":\n",
    "        continue\n",
    "    pd_min_tmp = pd_min.loc[:, label].to_frame().rename(columns={'date': 'date', label: label + \"min\"})\n",
    "    pd_tmp = pd_tmp.set_index(\"date\").join(pd_min_tmp).reset_index()\n",
    "    pd_max_tmp = pd_max.loc[:, label].to_frame().rename(columns={'date': 'date', label: label + \"max\"})\n",
    "    pd_tmp = pd_tmp.set_index(\"date\").join(pd_max_tmp).reset_index()\n",
    "    pd_tmp = pd_tmp.loc[110:120, :]\n",
    "    dev = [pd_tmp.loc[:, label] - pd_tmp.loc[:, label + \"min\"], pd_tmp.loc[:, label + \"max\"] - pd_tmp.loc[:, label]]\n",
    "    www_plot = plt.subplot(121)\n",
    "    plt.ylim(0, 0.3)\n",
    "    #plt.ylabel(\"A\")\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.errorbar(pd_tmp.date, pd_tmp.loc[:, label], yerr = dev, fmt='k-', ecolor='gray', lw=1)\n",
    "    #www_plot.xaxis.set_major_locator(xmajorLocator)\n",
    "    plt.show()\n",
    "\n",
    "print(pd_tmp.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first figure, we find the reputation score of facebook in recent 120 days, in which the x-axis presents the date timestamp, the y-axis indicates the index volume. Figure 1.1 is the change of positive reputation score.Figure 1.2 is the change of positive reputation score.Figure 1.3 is the change of positive reputation score.\n",
    "<img src=\"https://github.com/ShengjieLuo/ReputationFirst/raw/master/image/facebook.jpeg\" width=\"500\">\n",
    "In the second figure, we find the reputation score of google in recent 120 days,\n",
    "<img src=\"https://github.com/ShengjieLuo/ReputationFirst/raw/master/image/google.jpeg\" width=\"500\">\n",
    "In the third figure, we find the reputation score of Amazon in recent 120 days,\n",
    "<img src=\"https://github.com/ShengjieLuo/ReputationFirst/raw/master/image/amazon.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Case Analysis\n",
    "In this user case, we find a reputation increasing of facebook in Apr.28, the reason of which is that facebook begins a corporation with UK government in data protection.\n",
    "\n",
    "Following figure shows the reputation score of facebook, in which the blue curve indicates the positive index, and the orange curve indicates the negatice index. Obviously, the blue curve in this period has an positive news peek.\n",
    "<img src=\"https://github.com/ShengjieLuo/ReputationFirst/raw/master/image/case1.jpg\" width=\"500\">\n",
    "\n",
    "To find the true reason of this reputation increasing, we dive into the raw text and find that most media agencies reported that Facebook begins a coorperation with UK government in data protection to avoid the data leackage occuring again.\n",
    "\n",
    "For example, the following figure is a screenshot of Reuters' news in Apr.29 about facebook issue. It directly displays the willing of facebook to protect user privacy in the future, which is a positive report for facebook.\n",
    "<img src=\"https://github.com/ShengjieLuo/ReputationFirst/raw/master/image/case2.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion\n",
    "\n",
    "\"Reputation First\" is a public opinion research based on media articles. As we all know, Facebook is experiencing the largest reputation crisis recently, leading to millions of losses. Today, the public reputation matters for every company. Thatâ€™s also why we want to dive into social media data and try to predict reputation from them.\n",
    "\n",
    "Now, We are looking forward to extending this project into a **realtime analysis tool** in the future. Thanks for your attention!\n",
    "\n",
    "# Reference\n",
    "+ [1] Newspaper3k: Article scraping & curation https://github.com/codelucas/newspaper\n",
    "+ [2] Hamborg, Felix & Meuschke, Norman & Breitinger, Corinna & Gipp, Bela. (2017). news-please: A Generic News Crawler and Extractor. \n",
    "+ [3] Summa â€“ Textrank https://github.com/summanlp/textrank\n",
    "+ [4] Barrios, Federico & LÃ³pez, Federico & Argerich, Luis & Wachenchauzer, Rosa. (2016). Variations of the Similarity Function of TextRank for Automated Summarization. Proc. Argentine Symposium on Artificial Intelligence, ASAI.\n",
    "+ [5] Mihalcea, Rada & Tarau, Paul. (2004). TextRank: Bringing Order into Text.\n",
    "+ [6] Perkins, J. (2010). Text Classification for Sentiment Analysis â€“ Naive Bayes Classifier. [online] StreamHacker. Available at: http://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/ [Accessed 5 Apr. 2016].\n",
    "+ [7] Perkins, J. (2010). Text Classification for Sentiment Analysis â€“ Precision and Recall. [online] StreamHacker. Available at: http://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/ [Accessed 5 Apr. 2016].\n",
    "+ [8] \n",
    "Peter Buell Hirsch, (2017) \"Counting the spoons: what really influences corporate reputation\", Journal of Business Strategy, Vol. 38 Issue: 6, pp.54-58, https://doi.org/10.1108/JBS-09-2017-0131\n",
    "+ [9] \n",
    "Grahame Dowling, (2016) \"Defining and measuring corporate social reputations\", Annals in Social Responsibility, Vol. 2 Issue: 1, pp.18-28, https://doi.org/10.1108/ASR-08-2016-0008\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
