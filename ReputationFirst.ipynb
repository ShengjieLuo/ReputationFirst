{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reputation First\n",
    "## Public Opinion Research based on Media Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Internet Crawler\n",
    "In this project, we use distributed Internet Spider to crawl media articles from different sources. However, it is not easy to crawl the history data from media website directly for following reasons.\n",
    "\n",
    "+ Not only article links are listed in the website, also you could find the advertisement links, recommendation links and so on.\n",
    "+ The website content is customized based on userâ€™s habit and varies from different users in each single view.\n",
    "+ The anti-crawler is setup for these popular websites to avoid an overflow network traffic.\n",
    "\n",
    "Based on the observation above, we have to give up the attempts of fetching contents from the original websites.\n",
    "\n",
    "However, it is important to note that saome popular newspapers and magazines provide the **archive database** for users publicly, which is usually the printable and readable version of articles. Compared with the website content, it provides following features:\n",
    "+ It provides all articles published without customer bias.\n",
    "+ There is no or much less advertisement and other links.\n",
    "+ The content could be fetched from the html webpage directly by beautiful soup\n",
    "+ Usually, the content is marked with clear timestamps.\n",
    "\n",
    "Following are the archives for popular medias, we take the content from May.01 as an example here. Note that usually only the traditional media provides the archive as a summarization of past articles, while internet media or web media does not provide similar service.\n",
    "\n",
    "**TypeA: Newspaper & Maganizes**\n",
    "\n",
    "*Newyork Times*\n",
    "  + http://spiderbites.nytimes.com/2018/articles_2018_05_00000.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "\n",
    "*Wall Street Journals*\n",
    "  + http://www.wsj.com/public/page/archive-2018-5-01.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "\n",
    "*Washington Post*\n",
    "  + http://www.washingtonpost.com/wp-adv/archives/copyright.htm\n",
    "  + The archive is not timestamped, instead key word or topic based.\n",
    "\n",
    "**TypeB: News Agency**\n",
    "\n",
    "*BBC*: \n",
    "  + http://dracos.co.uk/made/bbc-news-archive/2018/05/01/\n",
    "  + Collected by third-party organization with less reliability. Also it does not include all contents of the media.\n",
    "  \n",
    "*Reuters*\n",
    "  + https://uk.reuters.com/resources/archive/uk/20180501.html\n",
    "  + The database is updated each day to provide the articles officially. Able to be crawled.\n",
    "  \n",
    "*CNN*\n",
    "  + CNN provides clear archive before 2001 with good format, however, the following materials are really confused without spicification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newspaper crawler is shown as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import threading\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NewsParser: The Newspaper Crawler Class\n",
    "'''\n",
    "class NewsParser(threading.Thread):\n",
    "\n",
    "    def __init__(self,source):\n",
    "        '''\n",
    "        Init Function:\n",
    "        1. Iniralization as a python thread\n",
    "        2. Define the source used in the parser\n",
    "        '''\n",
    "        threading.Thread.__init__(self)\n",
    "        self.source = source\n",
    "\n",
    "    def _fetch_links(self,url):\n",
    "        '''\n",
    "        1. Fetch the url links from archive database\n",
    "        2. Parse the database page to get the article links\n",
    "        '''\n",
    "        links = []\n",
    "        r = requests.get(url)\n",
    "        if r.status_code!=200:\n",
    "            print(url+\":Not Correct Response\")\n",
    "        soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "        for link in soup.find_all('a'):\n",
    "            rawlink = link.get('href')\n",
    "            if re.match(\"(\\S)+://www.nytimes.com/20(\\d)+/(\\d)+/(\\d)+/(\\S)+\\.html\",rawlink):\n",
    "                links.append(rawlink)\n",
    "        return links\n",
    "\n",
    "    def _fetch_data(self,link):\n",
    "        '''\n",
    "        1. Get the articles and parse the articles by \"newspaper\" lib\n",
    "        2. Add the realted field into the datum struct\n",
    "        '''\n",
    "        datum = {}\n",
    "        article = Article(link)\n",
    "        try:\n",
    "            article.download()\n",
    "        except:\n",
    "            print(link+\": Cannot be download!\")\n",
    "            return datum\n",
    "        try:       \n",
    "            article.parse()\n",
    "        except:\n",
    "            print(link+\": Cannot be parsed!\")\n",
    "            return datum            \n",
    "        datum[\"authors\"] = article.authors\n",
    "        datum[\"date\"]    = str(article.publish_date)\n",
    "        datum[\"text\"]    = str(article.text)\n",
    "        datum[\"title\"]   = str(article.title)\n",
    "        return datum\n",
    "    \n",
    "    def parse(self,source):\n",
    "        '''\n",
    "        Main Function to use the crawling function\n",
    "        '''\n",
    "        f = open(\"data_\"+source.split(\"/\")[-1],\"a\")\n",
    "        links = self._fetch_links(source)\n",
    "        count = 0\n",
    "        for link in links:\n",
    "            datum = self._fetch_data(link)\n",
    "            if len(datum)==0:\n",
    "                continue\n",
    "            f.write(json.dumps(datum)+'\\n')\n",
    "            print(\"Fetch news from \" + source + \" : \"+str(count)+\"/\"+str(len(links)))\n",
    "            count += 1\n",
    "        f.close()\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Provide the interactive interface as a python thread\n",
    "        '''\n",
    "        self.parse(self.source)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program shown above is a single thread internet crawler, however, the single thread crawler is too slower to crawl all newspaper articles. Following codes extend it into a multi-thread crawler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_targets():\n",
    "    '''\n",
    "    Read the \"sourcelist\" file to get the crawling resources    \n",
    "    '''\n",
    "    f = open(\"sourcelist\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            result.append(line.strip())\n",
    "    return result\n",
    "\n",
    "def real_main():\n",
    "    '''\n",
    "    1. Intializes multi-threads based on sources, each of which uses one thread\n",
    "    2. Coordinate multiple threads and wait for working threads end.\n",
    "    '''\n",
    "    sources = _get_targets()\n",
    "    threads = []\n",
    "    for source in sources:\n",
    "        threads.append(NewsParser(source))\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "real_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning by Spark (ETL)\n",
    "\n",
    "In the next step, we use Spark to clean the raw dataset fetched from the crawler following these steps:\n",
    "\n",
    "*Step1.* Filter out all articles of which **the company name is not included**.  Note that we match the company name by python regular expression within a case-insensitve style.\n",
    "\n",
    "*Step2.* For articles remained, we parse the articles to get the **title**, **summary** and **tag** fields. The method to extract the summary from original text would be described in the next chapter.\n",
    "\n",
    "*Step3.* Filter out articles which the company name is not included in title, summary and tag fields.\n",
    "\n",
    "It is important to note that,\n",
    "+ Sometimes the company name has different meanings, for example, the company name *\"Adobe\"* is used to describe a type of house as well which is used in some articles about earthquakes. Therefore, we use the summary function to identify the relation between articles and company names.\n",
    "+ The summary extraction is a time-consuming task with complex algorithm. Hence, we use step1 as a croase filtering to reduce the overall workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Coarse-Grained Data Filter\n",
    "\n",
    "Coarse Data Filter is used to filter out the articles which is not related to the companies in the company list.First, we use nltk to tokenize text into a list of tokens. And then, we compare the tokens with the company list one by one. Finally, we add the tag name into the result stuct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def _init_line(line):\n",
    "    name = line.lower().split()[0]\n",
    "    return (name,line.lower().split())\n",
    "\n",
    "def _init_list(sc):\n",
    "    results = {}\n",
    "    companyRDD = sc.textFile(\"gs://group688/companylist\")\n",
    "    coms = companyRDD.map(_init_line).collect()\n",
    "    for com in coms:\n",
    "        for name in com[1]:\n",
    "              results[name] = com[0]\n",
    "    return results   \n",
    "\n",
    "def _data_filter(lines,company,source):\n",
    "    import nltk\n",
    "    nltk.download('punkt',download_dir='./nltk_data')\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "    results = []\n",
    "    for datum in lines:\n",
    "        data    = json.loads(datum)\n",
    "        authors = data[\"authors\"]\n",
    "        date    = data[\"date\"]\n",
    "        text    = data[\"text\"]\n",
    "        title   = data[\"title\"]\n",
    "        tokens_text  = word_tokenize(text.lower())\n",
    "        tokens_title = word_tokenize(title.lower())\n",
    "        tags = []\n",
    "        for word in text.lower().split():\n",
    "            if word[0]==\"#\":\n",
    "            tags.append(word.lower())\n",
    "        #Stat is a dictionary, key is the company name, and value is the attribute\n",
    "        #attributes: [in_title,title_count,total_count]\n",
    "        stat  = {}\n",
    "        for token in tokens_title:\n",
    "              if token in company:\n",
    "                if company[token] in stat:\n",
    "                    stat[company[token]][0] = True\n",
    "                    stat[company[token]][1] += 1\n",
    "                else:\n",
    "                    stat[company[token]] = [True,1,0]\n",
    "        for token in tokens_text:\n",
    "              if token in company:\n",
    "                if company[token] in stat:\n",
    "                      stat[company[token]][2] += 1\n",
    "                else:\n",
    "                      stat[company[token]] = [False,0,1]\n",
    "        for name in stat:\n",
    "            result = {}\n",
    "            if (source==\"wsj\"):\n",
    "                result[\"date\"]      = date[:5] + '0' + date[5:9]\n",
    "            else:\n",
    "                result[\"date\"]      = date[:10]\n",
    "        result[\"text\"]        = text\n",
    "        result[\"tokens\"]      = tokens_text\n",
    "        result[\"company\"]     = name\n",
    "        result[\"source\"]      = source\n",
    "        result[\"in_title\"]    = stat[name][0]\n",
    "        result[\"title_count\"] = max(stat[name][1],title.lower().count(name))\n",
    "        result[\"total_count\"] = max(stat[name][2],text.lower().count(name))\n",
    "        result[\"title\"]       = title\n",
    "        result[\"authors\"]     = authors\n",
    "        result[\"tags\"]        = tags\n",
    "        results.append((name,json.dumps(result)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we introduced, even the coarse data filter is really time-consuming. In our experiment, we use a single thread data filter running on 300MB raw dataset with 255k articles, and it takes more than 2 hours to complete. To accelerate the execution time in a GB-level dataset, we use the Spark+Yarn platform to execute the program in parallel on a 5 instance cluster.\n",
    "The distributed cluster configuration is:\n",
    "+ **1 Master Node **\n",
    "  1. 4   CPUs\n",
    "  2. 16  GB memory\n",
    "  3. 100 GB storage\n",
    "  \n",
    "+ **4 Worker Nodes **\n",
    "  1. 2  CPUs\n",
    "  2. 12 GB memory\n",
    "  3. 80 GB storage\n",
    " \n",
    " The spark program is included as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_main():\n",
    "    sc = pyspark.SparkContext()\n",
    "    company = _init_list(sc)\n",
    "    dataRDD1 = sc.textFile(\"gs://group688/nytimes\",5)\n",
    "    dataRDD1 = dataRDD1.mapPartitions(lambda x:_data_filter(x,company,\"nytimes\"))\n",
    "    dataRDD2 = sc.textFile(\"gs://group688/wsj\",10)\n",
    "    dataRDD2 = dataRDD2.mapPartitions(lambda x:_data_filter(x,company,\"wsj\"))\n",
    "    dataRDD3 = sc.textFile(\"gs://group688/reuters.dat\",10)\n",
    "    dataRDD3 = dataRDD3.mapPartitions(lambda x:_data_filter(x,company,\"reuters\"))\n",
    "    dataRDD  = dataRDD3.union(dataRDD2).union(dataRDD1)\n",
    "    dataRDD.sortByKey().map(lambda x:x[1]).saveAsTextFile(\"gs://group688/688v1\")\n",
    "\n",
    "real_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support the Spark execution on cloud platform, we uses the **Google Cloud Dataproc** service to deply the spark cluster efficiently. Following is the scipt to combine the spark program with the google dataproc cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil rm -r gs://group688/688v1\n",
    "gcloud dataproc jobs submit pyspark \\\n",
    "--cluster spark688 \\\n",
    "--region us-east1 \\\n",
    "etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Generate Article Summary\n",
    "\n",
    "The second step is to generate the summary from the article, which helps further data cleaning. The principle and implementation of summary extraction would be introduced in next chapter. It also uses spark cluster to accelerate the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords\n",
    "def generate_summary(text):\n",
    "    abstract  = summarizer.summarize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Fine-Grained Data Filter\n",
    "\n",
    "The fine-grained data filter is the data filter based on the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pyspark\n",
    "\n",
    "def get_result_list(lines):\n",
    "    from summa import summarizer\n",
    "    from summa import keywords\n",
    "    import nltk\n",
    "    nltk.download('punkt',download_dir='./nltk_data')\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "    result_list = []\n",
    "    for line in lines:\n",
    "        json_data = json.loads(line)\n",
    "        text      = json_data[\"text\"]\n",
    "        abstract  = summarizer.summarize(text)\n",
    "        keyword   = keywords.keywords(text, split=True)\n",
    "        name = json_data['company']\n",
    "        tags_words = list(map(lambda x: x[1:], json_data['tags']))\n",
    "        abstract_words = list(map(lambda x: x.lower(), nltk.tokenize.word_tokenize(abstract)))\n",
    "        title_words = list(map(lambda x: x.lower(), nltk.tokenize.word_tokenize(json_data['title'])))\n",
    "        if abstract != '' and name not in abstract_words and name not in title_words and name not in tags_words:\n",
    "            continue\n",
    "        json_data['abstract'] = abstract\n",
    "        json_data['keywords'] = keyword\n",
    "        result_list.append(json.dumps(json_data))\n",
    "    return result_list\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    sc = pyspark.SparkContext()\n",
    "    dataRDD = sc.textFile(\"gs://group688/688v2.dat\",20)\n",
    "    dataRDD.mapPartitions(get_result_list).saveAsTextFile(\"gs://group688/688v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Generate Article Summary\n",
    "\n",
    "The second step is to generate the summary from the article, which helps further data cleaning. The principle and implementation of summary extraction would be introduced in next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
